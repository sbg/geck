# geck: Genotype Error Comparator Kit, for benchmarking genotyping tools
# Copyright (C) 2017 Seven Bridges Genomics Inc.

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

import numpy as np
from model import GeckModel


class GeckResults(GeckModel):
    """Analyzes N_complete samples produced by the solvers

    :ivar n_array_complete: instance of GeckSamples containing
                            samples of Ncomplete[G1,G2,g,m]
    :ivar n_confmatrix: dict of instances of GeckSamples containing
                        samples of n[i,j,k] confusion matrix
    :ivar precision: dict of dicts of dicts of precisions
                     (GeckSample instances)
                     e.g.
                      * ``precision["mother"]["soft"]["tool1"]`` is the
                        collection of samples of soft precision of
                        tool 1, evaluated on the mother's results,
    :ivar recall: dict of dicts of dicts of recalls
                  (GeckSample instances)
                  e.g.
                   * ``recall["family"]["hard"]["diff"]`` is the collection
                     of samples of
                     the hard
                     recall difference
                     between tool2 and tool 1, evaluated on the aggregate
                     results of all three family members
    :ivar f_score: dict of dicts of dicts of F scores
                   (GeckSample instances)
                   e.g.
                    * ``f_score["child"]["soft"]["diff"]`` is the collection
                      of samples of
                      the soft
                      f_score difference
                      between tool2 and tool 1, evaluated the child's results



    """
    def __init__(self, tool_names, n_array_complete_samples):
        """

        :param n_array_complete_samples:
            samples generated by a GeckSolver (list of numpy arrays)
        :type n_array_complete_samples: list

        """
        GeckModel.__init__(self)
        self.tool_names = tuple(tool_names)
        msg = 'n_array_complete_samples samples must be a ' \
              'list of numpy arrays of shape (27, 27, 15, 5)'
        assert isinstance(n_array_complete_samples, list), msg
        assert n_array_complete_samples[0].shape == (len(self.fg),
                                                     len(self.fg),
                                                     len(self.vfg),
                                                     len(self.modes)), msg

        self.n_array_complete = GeckSamples(n_array_complete_samples)
        self.n_confmatrix = {'father': None,
                             'mother': None,
                             'child': None,
                             'family': None}
        self.precision = {'father': None,
                          'mother': None,
                          'child': None,
                          'family': None}
        self.recall = {'father': None,
                       'mother': None,
                       'child': None,
                       'family': None}
        self.f_score = {'father': None,
                        'mother': None,
                        'child': None,
                        'family': None}

    def confusion_matrix(self, person='family'):
        """Calculates the joint confusion matrix

        Returns the joint confusion matrix (n_{i,j,k}) of the two tools
        as described by Eq. 17 in section 2.5.3 of Notes.pdf

        :param person: 'father' or 'mother' or 'child' or 'family'
        :type person: str
        :return: collection of confusion matrix samples
                 (numpy.arrays, shape (3,3,3)
        :rtype: GeckSamples

        """
        if person == 'family':
            k_array = self.k_array
        elif person == 'father':
            k_array = self.k_array_father
        elif person == 'mother':
            k_array = self.k_array_mother
        elif person == 'child':
            k_array = self.k_array_child
        else:
            raise ValueError('person must be \
                "father", "mother", "child" or "family"')
        if isinstance(self.n_confmatrix[person], type(None)):
            n_samples = []
            for n_array_complete in self.n_array_complete.samples:
                n_sample = \
                    np.einsum('GHgm,ijkgGH->ijk', n_array_complete, k_array)
                n_samples.append(n_sample)
            self.n_confmatrix[person] = GeckSamples(n_samples)
        return self.n_confmatrix[person]

    def _calculate_precision(self, n, mode):
        """Calculate precision from a 3x3 confusion matrix

        Calculate precision from a 3x3 (single-tool marginal) confusion matrix
        defined by:
        negative classes: "00"
        positive classes: "01" or "11"

        :param n: confusion counts, of shape 3x3
        :type n: numpy.array
        :param mode:
            * "hard": 0/1 <-> 1/1 mix-ups are not excused
            * "soft": 0/1 <-> 1/1 mix-ups are excused
            * "00": hard precision of variants called "00"
            * "01": hard precision of variants called "01"
            * "11": hard precision of variants called "11"
        :type mode: str
        :return: precision
        :rtype: float
        """
        if mode == 'hard':
            true_positive = np.sum(np.diag(n)[1:])
            called_positive = np.sum(n[:, 1:])
        elif mode == 'soft':
            true_positive = np.sum(n[1:, 1:])
            called_positive = np.sum(n[:, 1:])
        elif mode in self.ig:
            gt_idx = self.ig.index(mode)
            true_positive = n[gt_idx, gt_idx]
            called_positive = np.sum(n[:, gt_idx])
        else:
            raise ValueError("mode must be in " + str(self.ig) +
                             ", or in ['soft', 'hard']")
        return true_positive / float(called_positive)

    def _calculate_recall(self, n, mode):
        """Calculate recall from confusion matrix

        Calculate recall from a 3x3 (single-tool marginal) confusion matrix,
        defined by
        negative classes: "00"
        positive classes: "01" or "11"

        :param n: confusion counts, of shape 3x3
        :type n: numpy.array
        :param mode:
            "hard": 0/1 <-> 1/1 mix-ups are not excused
            "soft": 0/1 <-> 1/1 mix-ups are excused
            "00": hard recall of truly "00" variants
            "01": hard recall of truly "01" variants
            "11": hard recall of truly "11" variants
        :type mode: str
        :return: recall
        :rtype: float

        """
        if mode == 'hard':
            true_positive = np.sum(np.diag(n)[1:])
            positive = np.sum(n[1:, :])
        elif mode == 'soft':
            true_positive = np.sum(n[1:, 1:])
            positive = np.sum(n[1:, :])
        elif mode in self.ig:
            gt_idx = self.ig.index(mode)
            true_positive = n[gt_idx, gt_idx]
            positive = np.sum(n[gt_idx, :])
        else:
            raise ValueError("mode must be in " + str(self.ig) +
                             ", or in ['soft', 'hard']")
        return true_positive / float(positive)

    def _calculate_f_score(self, n, mode):
        """Calculate F score from confusion matrix

        Calculate F score from a 3x3 (single-tool marginal) confusion matrix,
        defined by
        negative classes: "00"
        positive classes: "01" or "11"

        :param n: confusion counts of shape 3x3
        :type n: numpy.array
        :param mode:
            "hard": 0/1 <-> 1/1 mix-ups are not excused
            "soft": 0/1 <-> 1/1 mix-ups are excused
            "00": hard recall of truly "00" variants
            "01": hard recall of truly "01" variants
            "11": hard recall of truly "11" variants
        :type n: str
        :return: F score
        :rtype: float

        """
        precision = self._calculate_precision(n, mode)
        recall = self._calculate_recall(n, mode)
        return 2 * (precision * recall) / (precision + recall)

    def _calculate_metrics_samples(self, metric_func, person='family'):
        """Returns a dict of dicts of the selected metric of the samples

        Levels of nested dictionary:
            lvl1: "soft" / "hard"
            lvl2: "tool1" / "tool2" / "diff" (i.e. tool2 - tool1 )

        :param metric_func: one of
            _calculate_precision/recall/Fscore()
        :type metric_func: function
        :param person: family member, one of
            ['father', 'mother', 'child', 'family']
        :type person: str
        :return: dict of dicts to be passed to self.p/r/F
        :rtype: dict

        """
        n_samples = self.confusion_matrix(person=person)
        metric = {}
        for mode in ("soft", "hard", "00", "01", "11"):
            metric[mode] = {}
            m1_samples = []
            m2_samples = []
            mdiff_samples = []
            for n in n_samples.samples:
                n1 = np.einsum('ijk->ij', n)
                n2 = np.einsum('ijk->ik', n)

                m1 = metric_func(n1, mode)
                m2 = metric_func(n2, mode)
                mdiff = m2 - m1
                m1_samples.append(np.array(m1))
                m2_samples.append(np.array(m2))
                mdiff_samples.append(np.array(mdiff))
            metric[mode]["tool1"] = GeckSamples(m1_samples)
            metric[mode]["tool2"] = GeckSamples(m2_samples)
            metric[mode]["diff"] = GeckSamples(mdiff_samples)
        return metric

    def get_precision(self, person='family'):
        """Returns a dict of dicts of precision samples

        :param person: family member,
            one of ['father', 'mother', 'child', 'family']
        :type person: str
        :return: a dict of dicts of precision samples.
                 Levels of nested dictionary:
                  * lvl1: "soft" / "hard"
                  * lvl2: "tool1" / "tool2" / "diff" (i.e. tool2 - tool1 )
                 e.g.
                 ``get_precision('child')['soft']['tool1']`` is the
                 soft precision of tool 1, evaluated on the child's results.

        :rtype: dict

        """
        if isinstance(self.precision[person], type(None)):
            self.precision[person] = self._calculate_metrics_samples(
                self._calculate_precision, person=person)
        return self.precision[person]

    def get_recall(self, person='family'):
        """Returns a dict of dicts of recall samples

        :param person: family member,
            one of ['father', 'mother', 'child', 'family']
        :type person: str
        :return: a dict of dicts of recall samples.
                 Levels of nested dictionary:
                  * lvl1: "soft" / "hard"
                  * lvl2: "tool1" / "tool2" / "diff" (i.e. tool2 - tool1 )
                 e.g.
                 ``get_recall('child')['soft']['tool1']`` is the
                 soft precision of tool 1, evaluated on the child's results.

        :rtype: dict

        """
        if isinstance(self.recall[person], type(None)):
            self.recall[person] = self._calculate_metrics_samples(
                self._calculate_recall, person=person)
        return self.recall[person]

    def get_f_score(self, person='family'):
        """Returns a dict of dicts of F score samples

        :param person: family member,
            one of ['father', 'mother', 'child', 'family']
        :type person: str
        :return: a dict of dicts of F score samples.
                 Levels of nested dictionary:
                  * lvl1: "soft" / "hard"
                  * lvl2: "tool1" / "tool2" / "diff" (i.e. tool2 - tool1 )
                 e.g.
                 ``get_f_score('child')['soft']['tool1']`` is the
                 soft precision of tool 1, evaluated on the child's results.

        :rtype: dict

        """
        if isinstance(self.f_score[person], type(None)):
            self.f_score[person] = self._calculate_metrics_samples(
                self._calculate_f_score, person=person)
        return self.f_score[person]

    def report_metrics(self,
                       mode='all',
                       person='all',
                       percentiles=(0.05, 0.95)):
        """Creates a human-readable report of the benchmarking metrics

        Including all modes, all persons, all benchmarking metrics, in a
        legible format,

        :param mode:
            * 'all'  - all 5 modes
            * 'soft' - 01<->11 errors are excused
            * 'hard' - 01<->11 errors count
            * '00'   - 00 is considered positive class (others negative)
            * '01'   - 01 is considered positive class (others negative)
            * '11'   - 11 is considered positive class (others negative)
        :type mode: str
        :param person: family member,
            one of ['father', 'mother', 'child', 'family']
        :type person: str
        :param percentiles: percentiles to be written in parenthesis
        :type percentiles: iterable
        :return: multi-line report with title
                 and empty lines for easy readability
        :rtype: str

        """
        available_modes = ('soft', 'hard', '00', '01', '11')
        available_persons = ('father', 'mother', 'child', 'family')
        report_lines = ['GECK report']
        title = 'Estimated benchmarking metrics [format: average, (' + \
            ', '.join([str(100 * q) + 'th' for q in percentiles]) + \
            ' percentiles)]'
        report_lines.append(title)
        report_lines.append('-' * len(title))
        report_lines.append('')

        metric_names = ['precision', 'recall', 'F score']
        metrics = [self.get_precision, self.get_recall, self.get_f_score]
        if mode == 'all':
            modes = available_modes
        else:
            assert mode in available_modes, 'mode must be "all" or one of ' \
                                            + str(available_modes)
            modes = [mode]
        if person == 'all':
            persons = available_persons
        else:
            assert mode in available_modes, 'mode must be "all" or one of ' \
                                            + str(available_persons)
            persons = [person]
        for person in persons:
            report_lines.append(person)
            report_lines.append('')
            for m in modes:
                for name, metric in zip(metric_names, metrics):
                    report_lines.append(name + ' (' + m + ')')
                    for tool in ('tool1', 'tool2', 'diff'):
                        report_lines.append(
                            tool + ': ' +
                            str(metric(person)[m][tool].avg()) +
                            ' (' +
                            ', '.join(
                                [str(metric(person)[m][tool].percentile(q))
                                 for q in percentiles]) +
                            ')')
                    report_lines.append('')
            report_lines.append('-' * len(title))
            report_lines.append('')
        return '\n'.join(report_lines)

    def dict_metrics(self, percentiles=(0.05, 0.95)):
        """Compiles a summary of metrics in dictionary format

        :param percentiles: percentiles to be reported
        :type percentiles: tuple
        :return: all metrics in nested dictionary form:
                  * level 1: person: 'family', 'father', 'mother', 'child'
                  * level 2: mode: 'hard', 'soft', '00', '01', '11'
                  * level 3: metric: 'precision', 'recall', 'F score'
                  * level 4: tool: 'tool1', 'tool2', 'diff'
                  * level 5: statistic: 'avg', 'stdev', 'percentiles'
                  * level 6: percentiles, (only if level 5 key
                    == 'percentiles')
                 e.g.
                 ``dict_metrics()['child']['soft']['F score']['diff']['avg']``
                 is the average of the soft F score difference between
                 the two tool, evaluated on the child's results
        :rtype: dict

        """
        modes = ('soft', 'hard', '00', '01', '11')
        tool_names = ('tool1', 'tool2', 'diff')
        persons = ('father', 'mother', 'child', 'family')
        metric_names = ('precision', 'recall', 'F score')
        metrics = (self.get_precision, self.get_recall, self.get_f_score)

        d = {}
        for person in persons:
            if person not in d:
                d[person] = {}
            for m in modes:
                if m not in d[person]:
                    d[person][m] = {}
                for name, metric in zip(metric_names, metrics):
                    if name not in d[person][m]:
                        d[person][m][name] = {}
                    for tool in tool_names:
                        if tool not in d[person][m][name]:
                            d[person][m][name][tool] = {}
                        d[person][m][name][tool]['avg'] = \
                            float(metric(person)[m][tool].avg())
                        d[person][m][name][tool]['stdev'] = \
                            float(np.sqrt(metric(person)[m][tool].var()))
                        for perc in percentiles:
                            if 'percentiles' not in d[person][m][name][tool]:
                                d[person][m][name][tool]['percentiles'] = {}
                            d[person][m][name][tool]['percentiles'][perc] = \
                                float(metric(person)[m][tool].percentile(perc))
        return d


class GeckSamples:
    """Storing and reports about a set of samples of one variable

    :ivar shape: the shape of the array of variables (tuple)
    :ivar samples: numpy array of shape (<number of samples>, self.shape)
    :ivar samples_sorted: same data where each column is sorted independently
                          (rows are mixed up!)
    :ivar avg_value: average of each column (numpy array of shape self.shape)
    :ivar var_value: variance of each column (numpy array of shape self.shape)
    :ivar percentile_value: dict of percentiles of each column
                            (dict of numpy arrays of shape self.shape)

    """
    def __init__(self, list_of_samples):
        """

        :param list_of_samples: list of numpy arrays of the same shape
        :type list_of_samples: list

        """
        self.samples = np.array(list_of_samples)
        self.shape = list_of_samples[0].shape
        total_entries = 1
        for dim in self.shape:
            total_entries *= dim
        self.samples_flat = self.samples.reshape(
            (len(list_of_samples), total_entries))
        self.samples_sorted = np.zeros_like(self.samples_flat)
        for i in range(self.samples_flat.shape[1]):
            self.samples_sorted[:, i] = np.sort(self.samples_flat[:, i])
        self.avg_value = None
        self.var_value = None
        self.percentile_value = {}

    def avg(self):
        """Returns average of a benchmarking metric

        Returns the average of each component in the shape of self.shape
        (return pre-computed value if exists)

        :return: average across samples
        :rtype: numpy.array

        """
        if isinstance(self.avg_value, type(None)):
            avg = np.einsum('si->i', self.samples_flat) \
                / float(self.samples.shape[0])
            self.avg_value = avg.reshape(self.shape)
        return self.avg_value

    def var(self):
        """Returns variance of a benchmarking metric

        Returns the variance of each component in the shape of self.shape
        (return pre-computed value if exists)

        :return: variance across samples
        :rtype: numpy.array

        """
        if isinstance(self.var_value, type(None)):
            var = np.einsum('si->i', self.samples_flat ** 2) / \
                float(self.samples.shape[0]) - \
                (np.einsum('si->i', self.samples_flat) /
                 float(self.samples.shape[0]))**2
            self.var_value = var.reshape(self.shape)
        return self.var_value

    def percentile(self, q):
        """Returns the (100*q)th percentile of a metric

        Returns the (100*q)th percentile of each component in the
        shape of self.shape
        (return pre-computed value if exists)

        :param q: percentile rank (between 0.0 and 1.0)
        :type q: float
        :return: percentile across samples
        :rtype: numpy.array

        """
        total = len(self.samples)
        rank = int(q * total)
        if rank >= total:
            rank = total - 1
        if rank not in self.percentile_value:
            self.percentile_value[rank] = \
                self.samples_sorted[rank, :].copy().reshape(self.shape)
        return self.percentile_value[rank]
